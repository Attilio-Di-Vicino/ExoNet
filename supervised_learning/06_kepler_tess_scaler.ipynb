{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 3 on kepler and TESS data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Added paths to import modules\n",
    "sys.path.insert(0, os.path.abspath('../'))\n",
    "\n",
    "# import custom modules\n",
    "from data_processing.distribution import (plot_class_distribution, plot_feature_importances,\n",
    "                                          plot_hist_feature_distributions,\n",
    "                                          plot_hist_feature_distributions_0_1,\n",
    "                                          compute_train_0_1)\n",
    "from data_processing.data_analysis import (compute_all_columns_nan, print_nan_numbers_for_features,\n",
    "                                           threshold_delete_nan)\n",
    "from data_processing.data_scaling import plot_top_7_difference, data_scaling_normalization\n",
    "from data_processing.features_prosessing import (remove_non_numeric_columns, rows_id_nan,\n",
    "                                                 remove_nan_label, feature_processing_kepler_tess)\n",
    "from data_processing.knn_imputer import k_nearest_neighbors_imputer\n",
    "from data_processing.replace_disposition import replace_label\n",
    "from dimensionality_reduction.features_selection import compute_feature_importance, feature_selection_rfc\n",
    "from model_selection.grid_search import grid_search_param_optimization\n",
    "from utils.mission import Mission\n",
    "from utils.util import print_count_nan, print_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data\n",
    "\n",
    "read the csv file taken from:\n",
    "1. Kepler: NASA Exoplanet Archive  http://exoplanetarchive.ipac.caltech.edu\n",
    "2. TESS: ExoFOP https://exofop.ipac.caltech.edu/tess/view_toi.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Kepler Cumulative KOI (9564, 141)\n",
      "Shape of ExoFOP TESS data: (6977, 62)\n",
      "Number of NaN values in Kepler Cumulative KOI: 237116 out of 1348524: 17.58%\n",
      "Number of NaN values in TESS: 38201 out of 432574: 8.83%\n"
     ]
    }
   ],
   "source": [
    "kepler_data = pd.read_csv('../data/raw_data/cumulative_2023.11.04_08.48.13.csv')\n",
    "tess_data = pd.read_csv('../data/raw_data/tess_exofop.csv')\n",
    "print('Shape of Kepler Cumulative KOI', kepler_data.shape)\n",
    "print('Shape of ExoFOP TESS data:', tess_data.shape)\n",
    "_ = print_count_nan(data=kepler_data, name='Kepler Cumulative KOI')\n",
    "_ = print_count_nan(data=tess_data, name='TESS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace Label\n",
    "\n",
    "In the data initially there is an disposition that includes 3 or more classes.\n",
    "\n",
    "Kepler:\n",
    "1. CONFIRMED\n",
    "2. CANDIDATE\n",
    "3. FALSE POSITIVE\n",
    "\n",
    "with the aim of making a binary classifier we use the following procedure to replace:\n",
    "1. CONFIRMED, CANDIDATE with the label: 1\n",
    "2. FALSE POSITIVE with the label: 0\n",
    "\n",
    "TESS: Disposizione TFOPWG\n",
    "1. APC = Candidato Planetario Ambiguo\n",
    "2. CP = Pianeta Confermato\n",
    "3. FA = Falso Allarme\n",
    "4. FP = Falso Positivo\n",
    "5. KP = Pianeta Conosciuto\n",
    "6. PC = Candidato Planetario\n",
    "\n",
    "with the aim of making a binary classifier we use the following procedure to replace:\n",
    "1. KP, CP, PC with the label: 1\n",
    "2. APC, FA, FP with the label: 0\n",
    "\n",
    "NASA Exoplanet Archive documentation: https://exoplanetarchive.ipac.caltech.edu/docs/API_TOI_columns.html \n",
    "\n",
    "ExoFOP documentation: https://exofop.ipac.caltech.edu/tess/tsm.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of kepler data:  (9564, 141)\n",
      "Shape of TESS data:  (6977, 62)\n"
     ]
    }
   ],
   "source": [
    "kepler_data = replace_label(data=kepler_data, mission=Mission.KEPLER)\n",
    "tess_data = replace_label(data=tess_data, mission=Mission.TESS)\n",
    "print('Shape of kepler data: ', kepler_data.shape)\n",
    "print('Shape of TESS data: ', tess_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature and Label Processing\n",
    "\n",
    "The Kepler and TESS data have different features, so we want to find an intersection between the two sets of data. To do this, we perform several operations:\n",
    "\n",
    "1. Removing non-numeric features\n",
    "2. Removing of identifying and follow-up characteristics\n",
    "3. Identifying the labels (X_train, y_train)\n",
    "4. Removing NaN in y_train and correspondingly also the X_train rows (if there are)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After the processing:\n",
      "Number of NaN values in TESS   : 22338 out of 181402: 12.31%\n",
      "Number of NaN values in Kepler : 7219 out of 229536: 3.15%\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = feature_processing_kepler_tess(kepler_data=kepler_data, tess_data=tess_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train data:  (16541, 26)\n",
      "Shape of y_train data:  (16541,)\n",
      "Number of NaN values in y_train: 98 out of 16541: 0.59%\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X_train data: ', X_train.shape)\n",
    "print('Shape of y_train data: ', y_train.shape)\n",
    "_ = print_count_nan(data=y_train, name='y_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train data:  (16443, 26)\n",
      "Shape of y_train data:  (16443,)\n",
      "Number of NaN values in X_train: 48359 out of 427518: 11.31%\n",
      "Number of NaN values in y_train: 0 out of 16443: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Removing NaN in y_train and correspondingly also the X_train rows\n",
    "X_train, y_train = remove_nan_label(X_train, y_train)\n",
    "count = y_train.isna().sum()\n",
    "print('Shape of X_train data: ', X_train.shape)\n",
    "print('Shape of y_train data: ', y_train.shape)\n",
    "_ = print_count_nan(data=X_train, name='X_train')\n",
    "_ = print_count_nan(data=y_train, name='y_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete columns\n",
    "name_columns_to_delete = ['RA', 'Dec', 'Epoch (BJD)', 'Epoch (BJD) err']\n",
    "X_train = X_train.drop(columns=name_columns_to_delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scaling\n",
    "\n",
    "Nonostante non costituisca un prerequisito necessario nei modelli di machine learning, il processo di normalizzare i dati Ã¨ tipicamente impiegato con l'obiettivo di:\n",
    "\n",
    "1. Standardizzare l'intervallo di valori di tutte le caratteristiche del dataset\n",
    "2. Migliorare la robustezza numerica degli algoritmi impiegati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = Normalizer()\n",
    "# X_train_normalized = scaler.fit_transform(X_train)\n",
    "# X_train = pd.DataFrame(X_train_normalized, columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = PowerTransformer()\n",
    "# X_train_normalized = scaler.fit_transform(X_train)\n",
    "# X_train = pd.DataFrame(X_train_normalized, columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = QuantileTransformer(output_distribution='uniform')\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train_normalized, columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors\n",
    "\n",
    "The NaN problem must be managed carefully, there are several solutions that can be used, one of these is the KNN methodology which calculates a distance (e.g. Euclidean or Manhattan) between the observations and calculates the new value to be inserted with some methodologies (e.g. IDWM or IRWM) but considering the nearest K, where K is defined a priori, generally for K the square root of N is considered where N is the number of observations, or an approximation is used.\n",
    "\n",
    "In this case, for efficiency reasons, the sklearn KNNImputer module was used which is optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in X_train: 47905 out of 361746: 13.24%\n",
      "Shape of X_train: (16443, 22)\n"
     ]
    }
   ],
   "source": [
    "_ = print_count_nan(data=X_train, name='X_train')\n",
    "print('Shape of X_train:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns containing only not a number in X_train: []\n",
      "Number of Observations: 16443\n",
      "NaN for each feature\n",
      "  1: Stellar Radius (R_Sun) err----> 11281\n",
      "  2: Stellar Radius (R_Sun)--------> 10031\n",
      "  3: Stellar Metallicity err-------> 6048\n",
      "  4: Stellar Metallicity-----------> 6047\n",
      "  5: Stellar log(g) (cm/s^2) err---> 2469\n",
      "  6: Stellar Mass (M_Sun) err------> 2342\n",
      "  7: Planet Radius (R_Earth) err---> 1819\n",
      "  8: Stellar Mass (M_Sun)----------> 1278\n",
      "  9: Stellar log(g) (cm/s^2)-------> 1152\n",
      " 10: Stellar Eff Temp (K) err------> 889\n",
      " 11: Planet Radius (R_Earth)-------> 830\n",
      " 12: Planet Equil Temp (K)---------> 642\n",
      " 13: Stellar Eff Temp (K)----------> 493\n",
      " 14: Planet Insolation (Earth Flux)> 476\n",
      " 15: Duration (hours) err----------> 467\n",
      " 16: Depth (ppm) err---------------> 460\n",
      " 17: Period (days) err-------------> 454\n",
      " 18: Depth (ppm)-------------------> 363\n",
      " 19: Planet SNR--------------------> 363\n",
      " 20: TESS Mag----------------------> 1\n",
      " 21: Period (days)-----------------> 0\n",
      " 22: Duration (hours)--------------> 0\n",
      "Total number of not a number in X_train: 47905\n"
     ]
    }
   ],
   "source": [
    "# Calculate columns that contain only nan and the number of nan for each columns\n",
    "nan_columns = compute_all_columns_nan(data=X_train)\n",
    "number_of_nan_columns = {col: X_train[col].isna().sum() for col in X_train}\n",
    "number_of_nan_columns = dict(sorted(number_of_nan_columns.items(), key=lambda x: x[1], reverse=True))\n",
    "print_nan_numbers_for_features(data=X_train, number_of_nan_columns=number_of_nan_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in X_train: 12029 out of 279531: 4.30%\n",
      "Shape of X_train: (16443, 17)\n"
     ]
    }
   ],
   "source": [
    "TCOL = 2400\n",
    "nan_columns = threshold_delete_nan(number_of_nan_columns=number_of_nan_columns,\n",
    "                                        nan_columns_name=nan_columns, threshold=TCOL)\n",
    "X_train = X_train.drop(columns=nan_columns)\n",
    "print_count_nan(data=X_train, name='X_train')\n",
    "print('Shape of X_train:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows >= of T=0.5: 2645 out of a total of 16443 By eliminating them you obtain 13798 observations\n"
     ]
    }
   ],
   "source": [
    "# Compute the number of not a number for each rows\n",
    "number_of_nan_rows = X_train.isna().sum(axis=1)\n",
    "TROW = 0.5\n",
    "id_rows = rows_id_nan(number_of_nan_rows=number_of_nan_rows,\n",
    "                      X_train=X_train, threshold=TROW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows >= T\n",
    "X_train = X_train.drop(id_rows).reset_index(drop=True)\n",
    "y_train = y_train.drop(id_rows).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (13798, 17)\n",
      "Number of NaN values in X_train: 0 out of 234566: 0.00%\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X_train:', X_train.shape)\n",
    "_ = print_count_nan(data=X_train, name='X_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in X_train: 0 out of 234566: 0.00%\n",
      "There are no NaN\n",
      "Number of not a number in X_train is: 0\n",
      "Shape of X_train data:  (13798, 17)\n",
      "Shape of y_train data:  (13798,)\n"
     ]
    }
   ],
   "source": [
    "INDEX_OF_K = 10\n",
    "# Using KNNImputer\n",
    "X_train = k_nearest_neighbors_imputer(X_train=X_train, index_of_k=INDEX_OF_K)\n",
    "count = X_train.isna().sum()\n",
    "print('Number of not a number in X_train is:', count.sum())\n",
    "print('Shape of X_train data: ', X_train.shape)\n",
    "print('Shape of y_train data: ', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train data:  (13798, 17)\n",
      "Shape of y_train data:  (13798,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X_train data: ', X_train.shape)\n",
    "print('Shape of y_train data: ', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "X_train.to_csv('../data/processed_data/X_kepler_tess_quantilet.csv')\n",
    "y_train.to_csv('../data/processed_data/y_kepler_tess_quantilet.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_tesi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
